{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['تشکیل شورایی ویژه دانشجویان ایرانی اوکراین/ وجود ۲۲۰ پروژه نیمه تمام بیمارستانی در کشور',\n",
       "       'خانه\\u200cتکانی اساسی و رونق دوباره شرکت\\u200cهای خدماتی',\n",
       "       'بازداشت وزرای فرهنگ و خارجه دولت مکلف لیبی در آستانه ادای سوگند',\n",
       "       ..., 'تساوی ارومیه و ایلام در دیدار معوقه لیگ فوتبال زنان',\n",
       "       'سهم استان مرکزی در طرح جهش تولید مسکن ۶۱ هزار واحد است',\n",
       "       'نظر متفاوت گروهی از محققان درباره نزدیکترین سیاهچاله به زمین'],\n",
       "      dtype='<U157')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = np.loadtxt('./title-scraper/titles.txt', encoding='utf-8', dtype='str', delimiter = '\\n')\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "titles_train, titles_test = train_test_split(titles, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size:  2187\n",
      "Test Set Size:  547\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Size: \", titles_train.shape[0])\n",
    "print(\"Test Set Size: \", titles_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SET_PATH = './dataset/titles_train.txt'\n",
    "TEST_SET_PATH = './dataset/titles_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TRAINING_SET_PATH, titles_train, encoding='utf-8', delimiter='\\n', fmt='%s')\n",
    "np.savetxt(TEST_SET_PATH, titles_test, encoding='utf-8', delimiter='\\n', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, PATH, is_test_set=0):\n",
    "        self.read_data(PATH)\n",
    "        self.clean_text()\n",
    "        self.tokenizer()\n",
    "        self.get_most_freq(is_test_set)\n",
    "        self.handle_unknown()\n",
    "        \n",
    "    \n",
    "    def read_data(self, PATH):\n",
    "        self.data = np.loadtxt(PATH, encoding='utf-8', dtype='str', delimiter='\\n')\n",
    "\n",
    "    def clean_text(self):\n",
    "        local_data = self.data\n",
    "        self.cleaned_data = []\n",
    "        for title in local_data:\n",
    "            title2 = re.sub(r'[^۰-۹آ-ی؟.\\s]','', title)\n",
    "            title2 = re.sub(r'[۰-۹]+', 'N', title2)\n",
    "            title2 = '\\s ' + title2 + ' \\e'\n",
    "            self.cleaned_data.append(title2)\n",
    "    \n",
    "    def tokenizer(self):\n",
    "       self.tokenized_data = list(map(lambda x: x.split(), self.cleaned_data))\n",
    "    \n",
    "    def get_most_freq(self, is_test_set):\n",
    "        frequencies = {}\n",
    "        n_tokens = 0\n",
    "        for title_list in self.tokenized_data:\n",
    "            for token in title_list:\n",
    "                if token in frequencies:\n",
    "                    frequencies[token] += 1\n",
    "                else:\n",
    "                    frequencies[token] = 1\n",
    "                n_tokens += 1\n",
    "\n",
    "        frequencies_pair_list = list(frequencies.items())\n",
    "        frequencies_pair_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if not is_test_set:\n",
    "            with open('frequent.txt', 'w', encoding='utf-8') as f:\n",
    "                most_frequencies_list = frequencies_pair_list[0:200]\n",
    "                tokens_string = ''\n",
    "                for token,value in most_frequencies_list:\n",
    "                    tokens_string += token + ',' + str(value) + '\\n'\n",
    "                f.write(tokens_string)\n",
    "\n",
    "        self.n_unique_tokens = len(frequencies_pair_list)\n",
    "        self.n_tokens = n_tokens\n",
    "        self.frequencies_dict = dict(frequencies_pair_list[0:10000])\n",
    "\n",
    "        return (self.frequencies_dict, self.n_tokens, self.n_unique_tokens)\n",
    "\n",
    "    def handle_unknown(self):\n",
    "        for i in range(len(self.tokenized_data)):\n",
    "            for j in range(len(self.tokenized_data[i])):\n",
    "                if self.tokenized_data[i][j] not in self.frequencies_dict:\n",
    "                    self.tokenized_data[i][j] = 'unk'\n",
    "        self.prepared_data = self.tokenized_data\n",
    "    \n",
    "    def info(self):\n",
    "        print(\"Number of Unique Tokens: \", self.n_unique_tokens)\n",
    "        sum_len = sum([len(title) for title in self.prepared_data])\n",
    "        print(\"Length of Sentences Average: {0:.2f}\".format(sum_len/len(self.prepared_data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramLanguageModel:\n",
    "    def __init__(self, data):\n",
    "        self.make_ngram_dicts(data)\n",
    "    \n",
    "    def make_ngram_dicts(self, data):\n",
    "        # unigram_dict\n",
    "        unigram_dict = {}\n",
    "        for title_list in data:\n",
    "            for token in title_list:\n",
    "                if token in unigram_dict:\n",
    "                    unigram_dict[token] += 1\n",
    "                else:\n",
    "                    unigram_dict[token] = 1\n",
    "        self.unigram_dict = unigram_dict\n",
    "\n",
    "        # bigram_dict\n",
    "        bigram_dict = {}\n",
    "        for title_list in data:\n",
    "            for i in range(1, len(title_list)):\n",
    "                if (title_list[i-1], title_list[i]) in bigram_dict:\n",
    "                        bigram_dict[(title_list[i-1], title_list[i])] += 1\n",
    "                else:\n",
    "                    bigram_dict[(title_list[i-1], title_list[i])] = 1\n",
    "        self.bigram_dict = bigram_dict\n",
    "\n",
    "        # trigram_dict\n",
    "        trigram_dict = {}\n",
    "        for title_list in data:\n",
    "            for i in range(2, len(title_list)):\n",
    "                if (title_list[i-2], title_list[i-1], title_list[i]) in trigram_dict:\n",
    "                        trigram_dict[(title_list[i-2], title_list[i-1], title_list[i])] += 1\n",
    "                else:\n",
    "                    trigram_dict[(title_list[i-2], title_list[i-1], title_list[i])] = 1\n",
    "        self.trigram_dict = trigram_dict               \n",
    "\n",
    "    def cal_2gram_prob(self, v1, v2, N_vocabs):\n",
    "        if (v1, v2) not in self.bigram_dict:\n",
    "            self.bigram_dict[(v1,v2)] = 0\n",
    "        if v1 not in self.unigram_dict:\n",
    "            self.unigram_dict[v1] = 0\n",
    "\n",
    "        return (self.bigram_dict[(v1, v2)] + 1)/(self.unigram_dict[v1] + N_vocabs)\n",
    "\n",
    "    def cal_3gram_prob(self, v1, v2, v3, N_vocabs):\n",
    "        if (v1, v2, v3) not in self.trigram_dict:\n",
    "            self.trigram_dict[(v1,v2,v3)] = 0\n",
    "        if (v1,v2) not in self.bigram_dict:\n",
    "            self.bigram_dict[(v1,v2)] = 0  \n",
    "        \n",
    "        return (self.trigram_dict[(v1, v2, v3)] + 1)/(self.bigram_dict[(v1, v2)] + N_vocabs)\n",
    "\n",
    "    def calculate_smoothed_probs(self, n, string):\n",
    "        N_vocabs = len(self.unigram_dict.keys())\n",
    "        vocabs = list(self.unigram_dict.keys())\n",
    "        probs_list = []\n",
    "        if n == 2:\n",
    "            for i in range(N_vocabs):\n",
    "                if vocabs[i] != '\\s':    \n",
    "                    probs_list.append((vocabs[i], self.cal_2gram_prob(string[-1], vocabs[i], N_vocabs)))\n",
    "        elif n == 3:\n",
    "            for i in range(N_vocabs):\n",
    "                if vocabs[i] != '\\s':\n",
    "                    probs_list.append((vocabs[i], self.cal_3gram_prob(string[-2], string[-1], vocabs[i], N_vocabs)))\n",
    "        \n",
    "        probs_list.sort(key=lambda x: x[1], reverse=True)\n",
    "        return probs_list[:5]\n",
    "        \n",
    "    def generate_text(self, n, input_string):\n",
    "        generated_token = None\n",
    "        if input_string[-1] == '\\e':\n",
    "            return None\n",
    "        while generated_token != '\\e':\n",
    "            most_prob_vocab = self.calculate_smoothed_probs(n, input_string)[0][0]\n",
    "            input_string += [most_prob_vocab]\n",
    "            generated_token = most_prob_vocab\n",
    "            if len(input_string) > 20:\n",
    "                input_string += ['\\e']\n",
    "                generated_token = '\\e'\n",
    "\n",
    "        return input_string    \n",
    "\n",
    "    def average_log_likelihood(self, sentences, n, N_eval):\n",
    "        sum = 0\n",
    "        N_vocabs = len(self.unigram_dict.keys())\n",
    "        for sentence in sentences:\n",
    "            sum += self.cal_2gram_prob(sentence[0], sentence[1], N_vocabs)\n",
    "            for i in range(2, len(sentence)):\n",
    "                if n == 2:\n",
    "                    sum += np.log2(self.cal_2gram_prob(sentence[i-1], sentence[i], N_vocabs))\n",
    "                elif n == 3:\n",
    "                    sum += np.log2(self.cal_3gram_prob(sentence[i-2], sentence[i-1], sentence[i], N_vocabs))\n",
    "        return sum/N_eval\n",
    "                \n",
    "    def evaluate_model(self, n, prepared_test_data, N_eval):\n",
    "        sentences = []\n",
    "        for title_list in prepared_test_data:\n",
    "            if len(title_list) <= 5:\n",
    "                sentence = self.generate_text(n, title_list[:-1])\n",
    "            else:    \n",
    "                sentence = self.generate_text(n, title_list[:5])\n",
    "            sentences.append(sentence)\n",
    "        return self.average_log_likelihood(prepared_test_data, n, N_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens:  712\n",
      "Length of Sentences Average: 13.10\n"
     ]
    }
   ],
   "source": [
    "dp_train = DataProcessor(TRAINING_SET_PATH)\n",
    "dp_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_model = NgramLanguageModel(dp_train.prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens:  508\n",
      "Length of Sentences Average: 12.93\n"
     ]
    }
   ],
   "source": [
    "dp_test = DataProcessor(TEST_SET_PATH, is_test_set=1)\n",
    "dp_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0567642788059604\n"
     ]
    }
   ],
   "source": [
    "ave_log_likelihood = ngram_model.evaluate_model(2, dp_test.prepared_data[:test_size], dp_test.n_tokens)\n",
    "print(ave_log_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.0456603274529344\n"
     ]
    }
   ],
   "source": [
    "ave_log_likelihood_3gram = ngram_model.evaluate_model(3, dp_test.prepared_data[:test_size], dp_test.n_tokens)\n",
    "print(ave_log_likelihood_3gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples of using NgramLanguageModel class methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\s', 'افتتاح', 'N', 'واحد', 'مسکونی', 'در', 'برابر', 'رشادت', 'آنان', 'است', '\\\\e']\n"
     ]
    }
   ],
   "source": [
    "completed_by_2gram = ngram_model.generate_text(2, dp_test.prepared_data[0][:5]) # This method return None if there is '\\e' at the last of input sentence.\n",
    "if completed_by_2gram:\n",
    "    print(completed_by_2gram)\n",
    "else:\n",
    "    print(\"It is a complete sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\\\e', 0.05154639175257732), ('سهم', 0.001288659793814433), ('استان', 0.001288659793814433), ('مرکزی', 0.001288659793814433), ('در', 0.001288659793814433)]\n"
     ]
    }
   ],
   "source": [
    "most_prob_vocabs = ngram_model.calculate_smoothed_probs(2, dp_test.prepared_data[0][:-1]) # Because of '\\e' character\n",
    "print(most_prob_vocabs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "312bcb4f056a3bc378f7c88d73c11ebba2eef788b7bc4248911e06b9e1383e75"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
